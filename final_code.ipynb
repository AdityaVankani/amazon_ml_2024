{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9607067,"sourceType":"datasetVersion","datasetId":5861674},{"sourceId":9607073,"sourceType":"datasetVersion","datasetId":5861676},{"sourceId":9613934,"sourceType":"datasetVersion","datasetId":5866692},{"sourceId":9633776,"sourceType":"datasetVersion","datasetId":5881731},{"sourceId":9633840,"sourceType":"datasetVersion","datasetId":5881787},{"sourceId":9633890,"sourceType":"datasetVersion","datasetId":5881829},{"sourceId":9643724,"sourceType":"datasetVersion","datasetId":5889275},{"sourceId":9660341,"sourceType":"datasetVersion","datasetId":5901975},{"sourceId":9660367,"sourceType":"datasetVersion","datasetId":5901998},{"sourceId":139501,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":118140,"modelId":141376}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install paddlepaddle paddleocr\n!python3 -m pip install paddlepaddle-gpu","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:39:03.613976Z","iopub.execute_input":"2024-10-19T07:39:03.614299Z","iopub.status.idle":"2024-10-19T07:40:19.414194Z","shell.execute_reply.started":"2024-10-19T07:39:03.614266Z","shell.execute_reply":"2024-10-19T07:40:19.413094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import paddle\nprint(paddle.utils.run_check())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:40:19.416195Z","iopub.execute_input":"2024-10-19T07:40:19.416520Z","iopub.status.idle":"2024-10-19T07:40:28.623648Z","shell.execute_reply.started":"2024-10-19T07:40:19.416486Z","shell.execute_reply":"2024-10-19T07:40:28.622764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch_xla==2.5.0\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:33:51.022926Z","iopub.execute_input":"2024-10-19T07:33:51.023901Z","iopub.status.idle":"2024-10-19T07:34:07.142761Z","shell.execute_reply.started":"2024-10-19T07:33:51.023865Z","shell.execute_reply":"2024-10-19T07:34:07.141531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\n# Initialize and set up the TPU\ndevice = xm.xla_device()  # Get the TPU device\nprint(f\"Using device: {device}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:34:17.421541Z","iopub.execute_input":"2024-10-19T07:34:17.422479Z","iopub.status.idle":"2024-10-19T07:34:17.427089Z","shell.execute_reply.started":"2024-10-19T07:34:17.422437Z","shell.execute_reply":"2024-10-19T07:34:17.426377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nimport requests\nimport cv2\nimport numpy as np\nfrom io import BytesIO\nfrom paddleocr import PaddleOCR\n\n# Initialize PaddleOCR\nocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True)\n\n# Load the CSV file\nfile_path = '/kaggle/input/amazon-train/train.csv'\ndf = pd.read_csv(file_path)\n\n# Function to download image and perform OCR using OpenCV\ndef extract_text_from_image(url):\n    try:\n        # Download the image\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # Check if the request was successful\n        \n        # Convert image content to NumPy array for OpenCV\n        img_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)  # Load the image using OpenCV\n        \n        # If image is not loaded properly, log an error\n        if img is None:\n            print(f\"Error loading image: {url}\")\n            return None\n        \n        # Use PaddleOCR to extract text\n        result = ocr.ocr(img)\n        \n        # Collect the text from OCR result\n        extracted_text = \" \".join([line[1][0] for line in result[0]])\n        return extracted_text\n\n    except requests.exceptions.RequestException as req_err:\n        print(f\"Request error processing {url}: {req_err}\")\n        return None\n\n    except Exception as e:\n        print(f\"Unexpected error processing {url}: {e}\")\n        return None\n\n# Iterate over the DataFrame and process each image in batches\nbatch_size = 1000  # Save progress after every 1000 iterations\nmax_images = 50000  # Maximum number of images to process from the starting point (50,001 to 100,000)\nstart_index = 100001  # Starting index\nend_index = 105001  # Ending index to avoid out-of-bounds error\nprocessed_images = 0  # Track the number of processed images\n\n# Adjust the loop to start from 50001 and end at 100000\nfor i in range(start_index, end_index, batch_size):\n    batch_df = df.iloc[i:i+batch_size]  # Process images in batches\n    batch_df['extracted_text'] = batch_df['image_link'].apply(extract_text_from_image)\n    \n    # Save the updated DataFrame to a CSV file after every batch\n    if processed_images == 0:  # For the first batch\n        batch_df.to_csv('test_temp.csv', mode='w', index=False, header=True)\n    else:\n        batch_df.to_csv('test_temp.csv', mode='a', index=False, header=False)\n\n    processed_images += len(batch_df)  # Update the number of processed images\n    print(f\"Processed {processed_images} entries\")\n\nprint(\"Processing complete!\")\n","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-18T12:31:08.867391Z","iopub.execute_input":"2024-10-18T12:31:08.868098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"print(\"hello world\")","metadata":{}},{"cell_type":"code","source":"print(\"hello world\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T16:59:27.769249Z","iopub.execute_input":"2024-10-15T16:59:27.769951Z","iopub.status.idle":"2024-10-15T16:59:27.775176Z","shell.execute_reply.started":"2024-10-15T16:59:27.769911Z","shell.execute_reply":"2024-10-15T16:59:27.774101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file with the extracted text\ncsv_file = '/kaggle/working/test_temp.csv'\ndf = pd.read_csv(csv_file)\n\n# Drop rows where the 'extracted_text' field is empty or NaN\ndf_cleaned = df[df['extracted_text'].notna() & df['extracted_text'].str.strip().astype(bool)]\n\n# Save the cleaned DataFrame to a new CSV file\ndf_cleaned.to_csv('cleaned_train_with_text.csv', index=False)\n\nprint(f\"Rows with non-empty 'extracted_text' field saved to 'cleaned_train_with_text.csv'.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-18T13:12:58.158321Z","iopub.execute_input":"2024-10-18T13:12:58.159269Z","iopub.status.idle":"2024-10-18T13:12:58.281826Z","shell.execute_reply.started":"2024-10-18T13:12:58.159225Z","shell.execute_reply":"2024-10-18T13:12:58.280903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers torch pandas\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T17:38:35.298683Z","iopub.execute_input":"2024-10-15T17:38:35.299701Z","iopub.status.idle":"2024-10-15T17:38:48.577686Z","shell.execute_reply.started":"2024-10-15T17:38:35.299655Z","shell.execute_reply":"2024-10-15T17:38:48.576530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\ncsv_file = '/kaggle/working/cleaned_train_with_text.csv'\ndf = pd.read_csv(csv_file)\n\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nprint(device)\ndef get_bert_embedding(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    cls_embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n    return cls_embedding\n\ndf['bert_embedding'] = df['extracted_text'].apply(lambda x: get_bert_embedding(x) if isinstance(x, str) else None)\n\ndf.to_csv('bert_embeddings_train_with_text.csv', index=False)\n\nprint(f\"BERT embeddings added and saved to 'bert_embeddings_train_with_text.csv'.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T13:13:15.490450Z","iopub.execute_input":"2024-10-18T13:13:15.491322Z","iopub.status.idle":"2024-10-18T13:14:55.165987Z","shell.execute_reply.started":"2024-10-18T13:13:15.491276Z","shell.execute_reply":"2024-10-18T13:14:55.164963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file = '/kaggle/working/bert_embeddings_train_with_text.csv'\ndf = pd.read_csv(csv_file)\n\n# Extract the image name from the image_link field (last part after '/')\ndf['image_name'] = df['image_link'].apply(lambda x: x.split('/')[-1] if isinstance(x, str) else None)\n\n# Save the DataFrame with the new image_name column\noutput_file = '/kaggle/working/train_with_image_name.csv'  # Save to working directory\ndf.to_csv(output_file, index=False)\n\nprint(f\"Image names added and saved to '{output_file}'.\")\n\n# After running the code, you can download the CSV file from the 'Output' section of the Kaggle notebook interface.\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T13:15:16.682822Z","iopub.execute_input":"2024-10-18T13:15:16.683829Z","iopub.status.idle":"2024-10-18T13:15:19.862345Z","shell.execute_reply.started":"2024-10-18T13:15:16.683780Z","shell.execute_reply":"2024-10-18T13:15:19.861400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install transformers if not already installed\n!pip install transformers\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T12:22:11.959858Z","iopub.execute_input":"2024-10-18T12:22:11.960178Z","iopub.status.idle":"2024-10-18T12:22:35.772754Z","shell.execute_reply.started":"2024-10-18T12:22:11.960139Z","shell.execute_reply":"2024-10-18T12:22:35.771873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForSequenceClassification\n\n# Specify the path where the model is saved\nmodel_path = '/kaggle/input/bart-base/pytorch/default/1/junu'\n\n# Load the BART tokenizer\ntokenizer = BartTokenizer.from_pretrained(model_path)\n\n# Load the BART model for sequence classification (or adjust for your task)\nmodel = BartForSequenceClassification.from_pretrained(model_path)\nmodel.eval()  # Set the model to evaluation mode\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T12:24:10.446821Z","iopub.execute_input":"2024-10-18T12:24:10.447236Z","iopub.status.idle":"2024-10-18T12:24:12.019092Z","shell.execute_reply.started":"2024-10-18T12:24:10.447197Z","shell.execute_reply":"2024-10-18T12:24:12.018052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text, tokenizer, max_length=128):\n    return tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T12:24:39.470105Z","iopub.execute_input":"2024-10-18T12:24:39.470583Z","iopub.status.idle":"2024-10-18T12:24:39.475499Z","shell.execute_reply.started":"2024-10-18T12:24:39.470540Z","shell.execute_reply":"2024-10-18T12:24:39.474631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file = '/kaggle/input/com-with-name/combined_train_with_image_name.csv'\ndf = pd.read_csv(csv_file)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:40:46.231366Z","iopub.execute_input":"2024-10-19T07:40:46.231860Z","iopub.status.idle":"2024-10-19T07:41:07.636997Z","shell.execute_reply.started":"2024-10-19T07:40:46.231821Z","shell.execute_reply":"2024-10-19T07:41:07.636198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:41:16.091846Z","iopub.execute_input":"2024-10-19T07:41:16.092574Z","iopub.status.idle":"2024-10-19T07:41:23.845050Z","shell.execute_reply.started":"2024-10-19T07:41:16.092532Z","shell.execute_reply":"2024-10-19T07:41:23.844263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\nfrom sklearn.model_selection import train_test_split\n\n# Initialize the tokenizer and model\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n\n# Example data\nX = df['combined_text']\ny = df['entity_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=104, test_size=0.25, shuffle=True)\n\ndef tokenize_function(text_list):\n    return tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n\ninput_tokens = [tokenize_function(x)['input_ids'].squeeze(0) for x in X_train]\ntarget_tokens = [tokenize_function(y)['input_ids'].squeeze(0) for y in y_train]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:51:44.196675Z","iopub.execute_input":"2024-10-19T07:51:44.197047Z","iopub.status.idle":"2024-10-19T07:53:03.833674Z","shell.execute_reply.started":"2024-10-19T07:51:44.196999Z","shell.execute_reply":"2024-10-19T07:53:03.832838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\n\n# Set the model to train mode\nmodel.train()\n\n# Send model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # Adjust learning rate if necessary\n\n# Convert tokenized data to tensors and move to GPU\ninput_tokens = [x.to(device) for x in input_tokens]\ntarget_tokens = [y.to(device) for y in target_tokens]\n\n# Define a basic training loop\nepochs = 20  # Adjust based on your needs\nbatch_size = 8  # Adjust batch size for performance\n\nfor epoch in range(epochs):\n    total_loss = 0\n    print(f'Epoch {epoch + 1}/{epochs}')\n\n    # Create a tqdm progress bar for the batch loop\n    for i in tqdm(range(0, len(input_tokens), batch_size), desc=\"Training Progress\", unit=\"batch\"):\n        # Prepare inputs and targets batch with padding\n        inputs_batch = pad_sequence(input_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id)\n        targets_batch = pad_sequence(target_tokens[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id)\n\n        optimizer.zero_grad()\n\n        # Forward pass: get model predictions\n        outputs = model(input_ids=inputs_batch, labels=targets_batch)\n\n        # Loss is calculated automatically by model (cross-entropy)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Backward pass: compute gradients and update weights\n        loss.backward()\n        optimizer.step()\n\n    # Display the average loss for this epoch\n    print(f'Epoch {epoch + 1} completed. Average Loss: {total_loss / len(input_tokens)}')\n\nprint(\"TrainingÂ complete\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:46:03.980424Z","iopub.execute_input":"2024-10-19T09:46:03.980722Z","iopub.status.idle":"2024-10-19T09:46:09.901797Z","shell.execute_reply.started":"2024-10-19T09:46:03.980690Z","shell.execute_reply":"2024-10-19T09:46:09.900646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Prepare test inputs\ninput_tokens_test = [tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)['input_ids'].squeeze(0).to(device) for text in X_test]\n\n# Initialize lists to store predicted and actual values\npredicted_values = []\nactual_values = list(y_test)  # Actual alphanumeric values for comparison\n\n# Batch size for evaluation\nbatch_size = 8\n\n# BLEU smoothing function\nsmooth_fn = SmoothingFunction().method4  # Handles cases where there is no overlap in n-grams\n\n# Initialize variables for BLEU and accuracy\ntotal_bleu_score = 0\ncorrect_predictions = 0\n\n# Loop through test data in batches\nfor i in tqdm(range(0, len(input_tokens_test), batch_size), desc=\"Evaluating\"):\n    inputs_batch = pad_sequence(input_tokens_test[i:i + batch_size], batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    # Generate predictions\n    with torch.no_grad():\n        generated_ids = model.generate(inputs_batch, max_length=20)\n\n    # Decode predictions to text\n    for idx, generated_id in enumerate(generated_ids):\n        generated_text = tokenizer.decode(generated_id, skip_special_tokens=True)\n        predicted_values.append(generated_text)\n\n        # Calculate BLEU score for this prediction (reference is the actual value from y_test)\n        actual_text = actual_values[i + idx]\n        reference = [actual_text.split()]  # BLEU expects a list of reference sentences (each as a list of words)\n        candidate = generated_text.split()  # Candidate translation\n\n        # Calculate BLEU score for this sample\n        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)\n        total_bleu_score += bleu_score\n\n        # Simple exact match comparison\n        if generated_text == actual_text:\n            correct_predictions += 1\n\n# Compute average BLEU score and accuracy\naverage_bleu_score = total_bleu_score / len(predicted_values)\naccuracy = correct_predictions / len(predicted_values)\n\n# Print a few examples with BLEU scores\nfor i in range(10):\n    print(f\"Predicted: {predicted_values[i]}, Actual: {actual_values[i]}\")\n    reference = [actual_values[i].split()]\n    candidate = predicted_values[i].split()\n    sample_bleu = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)\n    print(f\"BLEU score for sample {i}: {sample_bleu:.4f}\")\n\n# Print final evaluation metrics\nprint(f\"\\nExact Match Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Average BLEU Score: {average_bleu_score:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:46:16.673191Z","iopub.execute_input":"2024-10-19T09:46:16.674063Z","iopub.status.idle":"2024-10-19T09:59:24.710880Z","shell.execute_reply.started":"2024-10-19T09:46:16.674007Z","shell.execute_reply":"2024-10-19T09:59:24.709886Z"},"trusted":true},"execution_count":null,"outputs":[]}]}